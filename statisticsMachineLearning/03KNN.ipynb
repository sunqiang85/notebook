{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K近邻法\n",
    "K近邻法（k-nearest neighbor, k-NN）是一种基本分类与回归方法。K近邻算法1968年由Cover和Hart提出。\n",
    "\n",
    "## K近邻算法\n",
    "输入：训练数据集\n",
    "$$\n",
    "T=\\{ (x_1,y_1),(x_2,y_2), \\ldots (x_N,y_N)\\}\n",
    "$$\n",
    "输出：x所属的类y。\n",
    "1. 根据给定的距离度量，在训练集T中找出与x最邻近的k个点，涵盖这k个点的邻域记作$N_k(x)$\n",
    "2. 在$N_k(x)$中根据决策分类（多数表决）来决定x的类别y：\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "y=\\argmax_{c_j} \\sum_{x_i \\in N_k(x)} I(y_i=c_j)\\\\ \n",
    "$$\n",
    "其中$i=1,2,3,\\ldots,N, k=1,2,3,\\ldots,K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K近邻模型\n",
    "K近邻模型有三要素：距离度量，k值的选择和分类决策规则决定。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 距离度量\n",
    "$L_p$距离或Minkowski距离定义：\n",
    "设特征空间x是n维空间向量空间$R^d$\n",
    "$$\n",
    "x_i,x_j \\in \\mathcal{X} \\\\\n",
    "x_i=(x_i^1,x_i^2,\\ldots,x_i^d) \\\\\n",
    "x_j=(x_j^1,x_j^2,\\ldots,x_i^d) \n",
    "$$\n",
    "$x_i,x_j$的$L_p$距离定义为：\n",
    "$$\n",
    "L_p(x_i,x_j)=\\left( \\sum_{l=0}^d|x_i^l-x_j^l|^p \\right)^{\\frac{1}{p}}\n",
    "$$\n",
    "当$p=2$时，称为欧氏距离（Euclidean distance），即\n",
    "$$\n",
    "L_2(x_i,x_j)=\\left( \\sum_{l=0}^d|x_i^l-x_j^l|^p \\right)^{\\frac{1}{2}}\n",
    "$$\n",
    "当$p=1$时，称为曼哈顿距离（Manhatten distance），即\n",
    "$$\n",
    "L_1(x_i,x_j)=\\sum_{l=0}^d|x_i^l-x_j^l|\n",
    "$$\n",
    "当$p=\\infty$时,\n",
    "$$\n",
    "L_{\\infty}(x_i,x_j)=\\max_{l}|x_i^l-x_j^l|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K的取值\n",
    "K=1时选择最近的实例分类  \n",
    "K=N时，选择训练集中分类最多的那个分类  \n",
    "通常使用交叉验证法来选取最优的K值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类决策规则\n",
    "多数表决规则（majority voting rule）有如下解释：如果分类的损失函数为0-1损失函数，分类函数为\n",
    "$$\n",
    "f:R^n \\to \\{ c_1,c_2,\\ldots,c_k \\}\n",
    "$$\n",
    "那么误分类的概率为\n",
    "$$\n",
    "P(Y \\ne f(X))=1-P(Y=f(X))\n",
    "$$\n",
    "如果涵盖$N_k{x}$的区域的类别是$c_j$,误分率为\n",
    "$$\n",
    "\\frac{1}{k}\\sum_{x_i \\in {N_k(x)}}I(y_i \\ne c_j）=1-\\frac{1}{k}\\sum_{x_i \\in {N_k(x)}}I(y_i = c_j）\n",
    "$$\n",
    "要使误分率最小，即经验风险最小，就是要使$\\sum_\\limits{{x_i \\in {N_k(x)}}}I(y_i = c_j）$最大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K近邻的实现：kd树\n",
    "详见《统计学习方法》"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
