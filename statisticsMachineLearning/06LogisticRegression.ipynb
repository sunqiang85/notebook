{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑斯谛回归与最大熵模型\n",
    "逻辑斯谛回归（logistic regression）是统计学习中的经典学习方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logitstic distribution\n",
    "设X是连续随机变量，X服从logistic distribution，指X具有下列分布函数和密度函数：\n",
    "$$\n",
    "F(X)=P(X \\le x)=\\frac{1}{1+e^{-{(x-\\mu)}/{\\gamma}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x)=F'(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(1+e^{-(x-\\mu)/\\gamma})^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式中，$\\mu$为位置参数，$\\gamma$为形状参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分布函数$F(X)$关于$\\left( \\mu,\\frac{1}{2} \\right)$中心对称，呈S型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二项逻辑斯谛回归模型\n",
    "$$\n",
    "P(Y=1|X=x)=\\frac{\\exp{(w \\cdot x+b)}}{1+\\exp{(w \\cdot x +b)}} \\\\\n",
    "P(Y=0|X=x)=\\frac{1}{1+\\exp{(w \\cdot x +b)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个事件的几率(odds)是指该事件发生的概率与不发生概率的比值。如果该事件发生的概率为$p$，那么该事件的几率就是$\\frac{p}{1-p}$,该事件的对数几率(log odds)或logit函数是：\n",
    "$$\n",
    "logit(p)=\\log{\\frac{p}{1-p}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由前面公式可以推导得到\n",
    "$$\n",
    "\\log{\\frac{P(Y=1|x)}{1-P(Y=1|x)}}=w \\cdot x +b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在logistic回归模型中，输出$Y=1$的对数几率是输入$x$的线性函数。换一个角度，logistic模型将x的线性组合从$(-\\infty,\\infty)$转化到$(0,1)$来表示概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设\n",
    "$$\n",
    "P(Y=1|x)=\\pi(x) \\\\\n",
    "P(Y=0|x)=1-\\pi(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "似然函数为\n",
    "$$\n",
    "\\prod_{i=1}^N[\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数似然函数为\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "L(w) & = & \\sum_{i=1}^N y_i\\log{\\pi(x_i)}+(1-y_i)\\log{(1-\\pi(x_i)} \\\\\n",
    "&=& \\sum_{i=1}^N [y_i \\log{\\frac{\\pi(x_i)}{1-\\pi(x)}}+\\log{(1-\\pi(x))}]\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对$L(w)$求极大值，得到$w$的估计值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多项逻辑斯谛回归\n",
    "假设$Y \\in \\{ 1,2,\\ldots,K \\}$  \n",
    "当k属于$\\{ 1,2,\\ldots,K-1 \\}$\n",
    "$$\n",
    "P(Y=k,|x)=\\frac{\\exp(w_k \\cdot x)}{1+\\sum_{i=1}^{K-1}\\exp(w_i \\cdot x)}\n",
    "$$\n",
    "当$k=K$时\n",
    "$$\n",
    "P(Y=k,|x)=\\frac{1}{1+\\sum_{i=1}^{K-1}\\exp(w_i \\cdot x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最大熵模型\n",
    "最大熵模型（maxium entropy model）由最大熵原理推导实现\n",
    "## 最大熵原理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大熵原理认为，学习概率模型时，在所有的概率模型（分布）中，熵最大的模型时最好的模型。  \n",
    "假设离散随机变量\n",
    "$$\n",
    "H(P)=-\\sum_\\limits{x}P(x)\\log{P(X)}\n",
    "$$\n",
    "熵满足下列不等式\n",
    "$$\n",
    "0 \\le H(P) \\le \\log{|X|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式中，$|X|$表示$X$的取值个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大熵模型的定义\n",
    "给定一个训练数据集\n",
    "$$\n",
    "T=\\{(x_1,y_1),(x_2,y_2),\\ldots,(x_N,y_N)\\}\n",
    "$$\n",
    "可以确定联合分布P(X,Y)的经验分布：\n",
    "$$\n",
    "\\hat{P}(X=x,Y=y)=\\frac{v(X=x,Y=y)}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以确定边缘分布$P(X)$的经验分布：\n",
    "$$\n",
    "\\hat{P}(X=x)=\\frac{v(X=x)}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用特征函数（feature function）f(X,Y)来表示输入x与输出y之间的某一个事实\n",
    "$$\n",
    "\\begin{equation}\n",
    "f(x,y)=\n",
    "\\begin{cases}\n",
    "1,x与y满足某一事实\\\\\n",
    "0,否则\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征函数f(X,Y)关于经验分布$\\hat{P}(X,Y)$的期望期望值：\n",
    "$$\n",
    "E_{\\hat{P}}(f)=\\sum_\\limits{x,y}\\hat{P}(x,y)f(x,y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征函数f(X,Y)关于模型$P(Y|X)$与经验分布$\\hat{P}(X)$的期望值\n",
    "$$\n",
    "E_p(f)=\\sum_\\limits{x,y}\\hat{P}(x)P(y|x)f(x,y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果模型能够获取训练数据中的信息，那么就可以假设这两个期望值相等：\n",
    "$$\n",
    "E_{P}(f)=E_{\\hat{P}}(f)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或\n",
    "$$\n",
    "\\sum_\\limits{x,y}\\hat{P}(x)P(y|x)f(x,y)=\\sum_\\limits{x,y}\\hat{P}(x,y)f(x,y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将此作为模型学习的约束条件，加入有n个特征函数，就有n个约束条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大熵模型定义，假设满足所有约束条件的模型集合为：\n",
    "$$\n",
    "\\mathcal{C} \\equiv \\{ P \\in \\mathcal{P}|E_p(f_i)=E_{\\hat{P}}(f_i),i = 1,2,\\ldots,n \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义在条件概率分布$P(Y|X)$上的条件熵为：\n",
    "$$\n",
    "H(P)=-\\sum_\\limits{x,y}\\hat{P}(x)P(y|x)\\log{P(y|x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大熵模型的学习\n",
    "对于给定的训练数据集$T=\\{(x_1,y_1),(x_2,y_2),\\ldots,(x_N,y_N)\\}$，以及特征函数$f_i(X,Y),i=1,2,\\ldots,n$，最大熵模型等价于约束最优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\max_\\limits{P \\in \\mathcal{C}} H(P)=-\\sum_\\limits{x,y}\\hat{P}(x)P(y|x)\\log{P(y|x)} \\\\\n",
    "S.T. \\ E_p(f_i)=E_{\\hat{P}}(f_i) \\ ,i =1,2,\\ldots,n \\\\\n",
    "\\sum_\\limits{y}P(y|x)=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "等价于\n",
    "$$\n",
    "\\min_\\limits{P \\in \\mathcal{C}} -H(P)=\\sum_\\limits{x,y}\\hat{P}(x)P(y|x)\\log{P(y|x)} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先引入拉格朗日乘子$w_0,w_1,w_2,\\ldots,w_n$\n",
    "$$\n",
    "L(P,w)=-H(P)+w_0 ( 1-\\sum_yP(y|x) )+\\sum_{i=1}^n w_i(E_{\\hat{P}}(f_i)-E_p(f_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(P,w)=\\sum_{x,y}\\hat{P}(x)P(y|x)\\log{P(y|x)}+w_0(1-\\sum_y P(y|x))+\\sum_{i=1}^n w_i(\\sum_{x,y}\\hat{P}(x,y)f(x,y)-\\sum_{x,y}\\hat{P}(x)P(y|x)f(x,y))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最优化的原始问题是\n",
    "$$\n",
    "\\min_{P \\in C} \\max_w L(P,w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于$L(P,w)$是凸函数，等价于对偶问题：\n",
    "$$\n",
    "\\max_w \\min_{P \\in C}L(P,w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先$\\min_\\limits{P \\in C}L(P,w)$是关于$w$的函数\n",
    "$$\n",
    "\\Psi(w)=\\min_\\limits{P \\in C}L(P,w)=L(P_w,w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Psi(w)$称为对偶函数，将其解记作为\n",
    "$$\n",
    "P_w=\\arg \\min_{P \\in C}L(P,w)=P_w(y|x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求解过程，$L(P,w)$对$P(y|x)$的偏导数：\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{L(P,w)}}{\\partial P(y|x)}&=&\\sum_{x,y}\\hat{P}(x)(\\log P(y|x)+1)-\\sum_y w_0-\\sum_{x,y}(\\hat{P}(x)\\sum_{i=1}^nw_if_i(x,y)) \\\\\n",
    "&=& \\sum_{x,y}\\hat{P}(x)(\\log P(y|x)+1-w_0-\\sum_{i=1}^n w_i f_i(x,y))\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令偏导数等于0，解得\n",
    "$$\n",
    "P(y|x)=\\exp(\\sum_{i=1}^nw_i f_i(x,y)+w_0-1)=\\frac{exp(\\sum_{i=1}^nw_i f_i(x,y))}{\\exp(1-w_0)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于$\\sum_y P(y|x)=1$得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(y|x)=\\frac{1}{Z_w(x)}exp(\\sum_{i=1}^nw_i f_i(x,y))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中\n",
    "$$\n",
    "Z_w(x)=\\sum_yexp(\\sum_{i=1}^nw_i f_i(x,y))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后求解对偶问题的外部极大化：\n",
    "$$\n",
    "w^*=\\arg \\max_w \\Psi(w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 极大似然估计\n",
    "$$\n",
    "L_\\hat{P}(P_w)=\\log{\\prod_{x,y}P(y|x)^{\\hat P(x,y)}}=\\sum_{x,y}{\\hat{P}(x,y)}\\log{P(y|x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推导可得\n",
    "$$\n",
    "\\Psi(w)=L_{\\hat{P}}(P_w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型学习的最优化算法\n",
    "logistic回归模型、最大熵模型学习归结为似然函数为目标函数的最优化问题，通常通过迭代算法求解：\n",
    "- 迭代尺度法\n",
    "- 梯度下降法\n",
    "- 牛顿法或拟牛顿法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改进的迭代尺度法\n",
    "改进的迭代尺度法(improved iterative scaling, IIS)是一种最大熵模型学习的最优算法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
