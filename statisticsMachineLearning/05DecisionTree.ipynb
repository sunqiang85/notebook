{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "决策树（Decision Tree）是一种基本的分类和回归方法。可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。  \n",
    "决策树的学习时，有3个步骤：特征选择、决策树的生成与决策树的修剪。\n",
    "决策树学习的思想主要来源于Quinlan在1986年提出的ID3算法和1993年提出的C4.5算法。以及Breiman等人在1984年提出的CART算法。\n",
    "\n",
    "## 决策树模型与学习\n",
    "### 决策树模型\n",
    "决策树由有向边（directed edge）和结点（Node）组成。内部结点表示一个特征或属性，叶结点表示一个类\n",
    "\n",
    "### 决策树与if-then规则\n",
    "决策树可以看成是一个if-then规则的集合\n",
    "\n",
    "### 决策树与条件概率分布\n",
    "路径对应于划分单元，划分单元中某个类的实例比较多，则该路径结点偏向某一个类\n",
    "\n",
    "### 决策树学习\n",
    "决策树学习本质是从训练数据集中归纳出一组分类规则。选取最优决策树是NP完全问题，所以现实中通常采用启发式算法，得到近似最优解。为了更好的进行预测，需要进行减枝。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征选择\n",
    "### 信息增益\n",
    "在信息论与概率统计中，熵（entropy）是随机变量不确定性的度量，设X是取有限个值得离散随机变量。\n",
    "$$\n",
    "P(X=x_i)=p_i, \\  i=1,2,\\ldots,n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则随机变量X的熵定义为：\n",
    "$$\n",
    "H(X)=\\sum_{i=1}^n p_i \\log{p_i}\n",
    "$$\n",
    "以2为底的熵单位是比特(bit),以e（自然对数）为底的熵单位是纳特(nat)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益的算法：\n",
    "输入：训练数据集D和特征A\n",
    "输出：信息增益$g(D,A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1）计算数据集D的信息熵$H(D)$\n",
    "$$\n",
    "H(D)=-\\sum_{k=1}^K\\frac{|C_k|}{|D|}\\log_2 \\frac{|C_k|}{|D|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）计算特征A对数据集D的经验条件熵\n",
    "$$\n",
    "H(D|A)=\\sum_i^n \\frac{|D_i|}{|D|}H(D_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3）计算信息增益\n",
    "$$\n",
    "g(D,A)=H(D)-H(D|A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息增益比\n",
    "$$\n",
    "g_R(D,A)=\\frac{g(D,A)}{H(D)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的生成\n",
    "### ID3算法（略）\n",
    "### C4.5的生成算法（略）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的减枝\n",
    "设树T的叶结点个数为$|T|$,t是树T的叶结点，该结点有$N_t$个样本点，其中k类的样本点有N_{tk}个，$k \\in \\{1,2,\\ldots,K \\}$，$H_t(T)$为叶结点t上的经验熵，$\\alpha \\ge 0$为参数，则决策树学习的损失函数定义为\n",
    "$$\n",
    "C_{\\alpha}(T)=\\sum_{t=1}^T N_t H_t(T)+\\alpha |T|\n",
    "$$\n",
    "其中经验熵为\n",
    "$$\n",
    "H_t(T)=-\\sum_{k=1}^K \\frac{N_{tk}}{N_t} \\log {\\frac{N_{tk}}{N_t}}\n",
    "$$\n",
    "再定义$C(T)$为\n",
    "$$\n",
    "C(T)=-\\sum_{t=1}^T \\sum_{k=1}^K \\frac{N_{tk}}{N_t} \\log {\\frac{N_{tk}}{N_t}}\n",
    "$$\n",
    "这时有\n",
    "$$\n",
    "C_{\\alpha}(T)=C(T）+\\alpha |T|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART算法\n",
    "分类与回归树（classification and regression tree, CART）模型由Breiman等人在1984年提出，是应用广泛的决策树学习方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设X与Y分别为输入和输出变量，并且Y是连续变量，给定训练数据集：\n",
    "$$\n",
    "D=\\{(x_1,y_1),(x_2,y_2),\\ldots,(x_N,y_N)\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设输入空间已经划分为$R_1,R_2,\\ldots,R_M$个，对应固定输出值$c_1,c_2,\\ldots,c_M$  \n",
    "回归树模型可表示为：\n",
    "$$\n",
    "f(x)=\\sum_{m=1}^M c_mI(x \\in R_m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以用平方误差\n",
    "$$\n",
    "\\sum_{x_i \\in R_m}(y_i-f(x_i))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单元R_m上的最优值$\\hat{c}_m$是R_m上的所有实例$x_i$的输出值$y_i$的平均值，即\n",
    "$$\n",
    "\\hat{c}_m=ave(y_i|x_i \\in R_m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最小二乘回归树生成算法\n",
    "选择最优切分变量j与切分点s，求解\n",
    "$$\n",
    "\\min_\\limits{j,s} \\left[ \\min_\\limits{c_1} \\sum_\\limits{x_i \\in R_1(j,s)} (y_i-c_1)^2 + \\min_\\limits{c_2} \\sum_\\limits{x_i \\in R_2(j,s)} (y_i-c_2)^2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基尼指数\n",
    "分类问题中，假设有K个分类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为：\n",
    "$$\n",
    "Gini(p)=\\sum_\\limits{k=1}^K p_k(1-p_k)=1-\\sum_\\limits{k=1}^K {p_k}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
