{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提升方法 Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kearns和Valiant在概率近似正确PAC（probably approximately correct）学习的框架中：  \n",
    "强可学习（strongly learnable),一个概念，如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么称这个概念是强可学习的；  \n",
    "弱可学习（weakly learnable),一个概念，如果存在一个多项式的学习算法能够学习它，并且正确率仅比随机猜测略好，那么称这个概念是弱可学习的；  \n",
    "Schapire证明强可学习与弱可学习是等价的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：训练数据集$T=\\{(x_1,y_1),(x_2,y_2),\\ldots,(x_N,y_N)\\}$，其中$x_i \\in R^n , y_i \\in \\{-1,+1\\}$，弱学习算法  \n",
    "输出：最终分类器G(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)初始化训练数据的权值分布  \n",
    "$D_1=(w_11,w_12,\\ldots,w_1N), w_{1i}=\\frac{1}{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)对$m=1,2,\\ldots,M$  \n",
    "(a)使用具有权值分布D_m的训练数据集学习，得到基本分类器\n",
    "$$\n",
    "G_m(x):\\mathcal{x} \\rightarrow \\{-1,+1\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)计算$G_m(x)$在训练数据集上的分类误差\n",
    "$$\n",
    "e_m=P(G_m(x_i) \\ne y_i)=\\sum_{i=1}^N w_{mi} I(G(x_i) \\ne y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)计算$G_m(x)$的系数\n",
    "$$\n",
    "\\alpha_m=\\frac{1}{2}\\log{\\frac{1-e_m}{e_m}}\n",
    "$$\n",
    "这里的对数是自然对数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)更新训练数据集的权值分布\n",
    "$$\n",
    "D_{m+1}=(w_{m+1,1},\\ldots,w_{m+1,N}) \\\\\n",
    "w_{m+1,i}=\\frac{w_{mi}}{Z_m} \\exp(-\\alpha_m y_i G_m(x_i))\n",
    "$$\n",
    "这里，$Z_m$是规范化因子\n",
    "$$\n",
    "Z_m=\\sum_{i=1}^N w_{mi} \\exp(-\\alpha_m y_i G_m(x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3)构建基本分类器的线性组合\n",
    "$$\n",
    "f(x)=\\sum_{m=1}^M \\alpha_m G_m(x)\\\\\n",
    "G(x)=sign(f(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost 算法的训练误差分析\n",
    "AdaBoost算法最终分类器的训练误差界为\n",
    "$$\n",
    "\\frac 1 N \\sum_{i=1}^N I(G(x_i) \\ne y_i) \\le \\frac{1}{N} \\sum_{i=1}^N \\exp(-y_i f(x_i))=\\prod_m Z_m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二类分类问题AdaBoost的训练误差边界\n",
    "$$\n",
    "\\prod_{m=1}^M  Z_m=\\prod _{m=1}^M[2\\sqrt{e_m(1-e_m)}]=\\prod_{m=1}^M\\sqrt{1-4  {\\gamma_m}^2}=\\exp(-2\\sum_{m=1}^M {\\gamma_m}^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "证明\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "Z_m &=& \\sum_{i=1}^N w_{mi} \\exp(-\\alpha_m y_i G_m(x_i)) \\\\\n",
    "&=& \\sum_{y_i G_m(x_i)=1} w_{mi} \\exp(-\\alpha_m)+\\sum_{y_i G_m(x_i)=-1} w_{mi} \\exp(\\alpha_m) \\\\\n",
    "&=& \\sqrt{\\frac{e_m}{1-e_m}}\\sum_{y_i G_m(x_i)=1} w_{mi}+\\sqrt{\\frac{1-e_m}{e_m}} \\sum_{y_i G_m(x_i)=-1} w_{mi} \\\\\n",
    "&=& 2\\sqrt{e_m(1-e_m)} \\\\\n",
    "&=& \\sqrt{1-4 {\\gamma_m}^2}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "其中$\\gamma_m=\\frac 1 2 -e_m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果存在$\\gamma \\gt 0$,对于所有$m,\\gamma_m \\ge \\gamma$，那么\n",
    "$$\n",
    "\\frac 1 N \\sum_{i=1}^N I(G(x_i) \\ne y_i)  \\le \\exp(-2\\sum_{m=1}^M {\\gamma_m}^2) \\le \\exp(-2M{\\gamma}^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到AdaBoost的训练误差是以指数速率下降的。这里的Ada是Adaptive适应的意思，即它能自适应弱分类器各自的训练误差率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost算法的解释\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向分步算法 foward stagewise algorithm\n",
    "加法模型（additvie model）\n",
    "$$\n",
    "f(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数最小化：\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\min_{\\beta_m,\\gamma_m} \\sum_{i=1}^N L(y_i,\\sum_{m=1}^M \\beta_m b(x_i;\\gamma_m))\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向分步算法与AdaBoost\n",
    "前向分步算法的损失函数是损失函数（exponential loss function）\n",
    "$$\n",
    "L(y,f(x))=\\exp[-yf(x)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设经过m-1轮迭代前向分步算法已经得到$f_{m-1}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f_{m-1}(x)=f_{m-2}(x)+\\ldots+\\alpha_{m-1}G_{m-1}(x)=\\sum_{i=1}^{m-1} \\alpha_i G_i(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一次目标\n",
    "$$\n",
    "(\\alpha_m,G_m(x))=\\arg \\min_{\\alpha,G} \\sum_{i=1}^N \\exp(-y_i (f_{m-1}(x_i)+\\alpha G(x_i)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\alpha_m,G_m(x))=\\arg \\min_{\\alpha,G} \\sum_{i=1}^N \\bar{w}_{mi} \\exp(-y_i \\alpha G(x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分两步，第一步求$G_m^*(x)$：\n",
    "$$\n",
    "G_m^*(x)=\\arg \\min \\sum_{i=1}^N \\bar{w}_{mi} I(y_i \\ne G(x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步：\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\sum_{i=1}^N \\bar{w}_{mi} I(y_i \\ne G(x_i)) &=& \\sum_{y_i=G_m(x_i)}\\bar{w}_{mi} e^{-\\alpha}+\\sum_{y_i \\ne G_m(x_i)}\\bar{w}_{mi}e^\\alpha \\\\\n",
    "&=& (e^\\alpha -e^{-\\alpha})\\sum_{i=1}^N\\bar{w}_{mi}I(y \\ne G(x_i))+e^{-\\alpha}\\sum_{i=1}^N \\bar{w}_{mi}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对$\\alpha$求导得\n",
    "$$\n",
    "\\alpha_m^*=\\frac{1}{2}\\log{\\frac{1-e_m}{e_m}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提升树\n",
    "### 提升树模型\n",
    "提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。以决策树为基函数的提升方法称为提升树（decision tree）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J个分类的树可以表示为\n",
    "$$\n",
    "T(x,\\Theta)=\\sum_{j=1}^JI(x \\in R_j)\n",
    "$$\n",
    "其中T是基函数，$\\Theta$是参数，$R_j$是区域"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(y,f(x))=(y-f(x))^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray}\n",
    "L(y,f_{m-1}(x)+T(x;\\Theta) &=& (y-f_{m-1}(x)-T(x;\\Theta))^2 \\\\\n",
    "&=& (r-T(x;\\Theta))^2\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里$r=y-f_{m-1}(x)$是当前模型拟合数据的残差，对回归问题的提升算法来说，只需要简单的拟合当前模型的残差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f_M(x)=\\sum_{m=1}^M T(x;\\Theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度提升(略)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
