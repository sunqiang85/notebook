{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量机\n",
    "支持向量机（support vector machine）是一种二类分类模型。它的基本模型时定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机。支持向量机还包括核技巧，这使它成为实质上的非线性分类器。支持向量机的学习策略是间隔最大化，可形式化为一个求解凸二次规划（convex quadratic programming）的问题，也等价于正则化的合页损失函数的最小问题。\n",
    "Cortes与Vapnik提出线性支持向量机，Boser、Guyon与Vapnik又引入核技巧，提出非线性支持向量机。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性可分支持向量机与硬间隔最大化\n",
    "感知机利用误分类最小策略求分离超平面，存在无穷多个解。\n",
    "线性可分支持向量机利用间隔最大化求解最优分离超平面，解唯一。\n",
    "### 线性可分支持向量机\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定线性可分训练数据集，通过间隔最大化或等价的求解对应的凸二次规划问题学习得到的分离超平面为\n",
    "$$\n",
    "w^*\\cdot x+b^*=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以及相应的分类决策函数\n",
    "$$\n",
    "f(x)=sign(w^*\\cdot x+b^*)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**函数间隔**\n",
    "对于给定的训练数据集T与超平面(w,b)，定义超平面(w,b)关于样本点（x_i,y_i）的函数间隔为\n",
    "$$\n",
    "\\hat{\\gamma}_i=y_i(w_i\\cdot x_i +b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义$\\hat{\\gamma}$为关于所有样本点中的最小值\n",
    "$$\n",
    "\\hat{\\gamma}=\\min_{i=1,\\ldots,N}\\gamma_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**几何间隔 geometric margin**\n",
    "通过规范化法向量$w$，使得$||w||=1$,这时函数间隔称为几何间隔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大化间隔问题可写为\n",
    "$$\n",
    "\\max_{w,b} \\frac{\\hat \\gamma}{||w||} \\\\\n",
    "s.t. \\ y_i(w_i\\cdot x_i+b) \\ge \\hat{\\gamma}, i=1,2,\\ldots,N\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "等价于凸二次规划\n",
    "$$\n",
    "\\min_{w,b} \\frac{1}{2}||w||^2 \\\\\n",
    "s.t. y_i(w \\cdot x_i+b)-1 \\ge 0, i=1,2,\\ldots,N\n",
    "$$\n",
    "最大间隔分离超平面存在且唯一"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相关定义\n",
    "### 仿射函数\n",
    "$f(x)$称为仿射函数，如果它满足$f(x)=a\\cdot x+b,a \\in R^n, b \\in R, x\\in R^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 凸优化\n",
    "凸优化问题是指约束最优化问题\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\min_{w}f(w) \\\\\n",
    "s.t. g_i(w) & \\le & 0 , i=1,2,\\ldots,k \\\\\n",
    "h_i(w) & = & 0 , i=1,2,\\ldots,l\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "其中，目标函数$f(w)$和约束函数$g_i(w)$都是$R^n$上的连续可微函数，约束函数$h_i(w)$是$R^n$上的仿射函数  \n",
    "当目标函数$f(w)$为二次函数且$g_i(w)$是放射函数时，上述凸优化问题成为凸二次规划问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量\n",
    "在线性可分的情况下，训练数据集中的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector）。支持向量是是使约束条件成立的点。即\n",
    "$$\n",
    "y_i(w \\cdot x_i+b)-1=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$y_i=+1$的正例点，支持向量在超平面\n",
    "$$\n",
    "H1:w \\cdot x +b=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$y_i=-1$的负例点，支持向量在超平面\n",
    "$$\n",
    "H2:w \\cdot x+b=-1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H1与H2之间的距离称为**间隔(margin)**，等于$\\frac{2}{||w||}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习的对偶算法\n",
    "首先构建拉格朗日函数（Lagrange function）。为此，对每一个不等式约束引入拉格朗日乘子，定义拉格朗日函数：\n",
    "$$\n",
    "L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^N \\alpha_i y_i (w \\cdot x_i+b)+\\sum_{i=1}^N \\alpha_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始问题\n",
    "$$\n",
    "\\min_{w,b} \\max_{\\alpha} L(w,b,\\alpha)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对偶问题\n",
    "$$\n",
    "\\max_{\\alpha} \\min_{w,b} L(w,b,\\alpha)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性可分支持向量机学习算法\n",
    "略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性支持向量机与软间隔最大化\n",
    "对线性不可分训练数据，引入松弛变量$\\xi_i$，约束条件变为：\n",
    "$$\n",
    "y_i(w \\cdot x_i +b) \\ge 1-\\xi_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标函数变为：\n",
    "$$\n",
    "\\frac{1}{2}||w||^2+C\\sum_{i=1}^N \\xi_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对应凸二次规划\n",
    "$$\n",
    "\\min_{w,b,\\xi} \\frac{1}{2}||w||^2+C\\sum_{i=1}^N \\xi_i \\\\\n",
    "s.t. \\quad y_i(w \\cdot x_i+b) \\ge 1- \\xi_i, \\quad i=1,2,\\ldots,N\\\\\n",
    "\\xi_i \\ge 0, \\quad i=1,2,\\ldots,N\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以证明$w$的解是唯一的，但b的解不唯一,b的解存在于一个区间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对偶问题**\n",
    "（转换后的对偶问题）\n",
    "$$\n",
    "\\min_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) -\\sum_{i=1}^N \\alpha_i \\\\\n",
    "s.t. \\quad \\sum_{i=1}^N \\alpha_i y_i=0 \\\\\n",
    "0 \\le \\alpha_i \\le C, \\quad i=1,2,\\ldots,N\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始最优化问题的拉格朗日函数是：\n",
    "$$\n",
    "L(w,b,\\xi,\\alpha,\\mu)=\\frac{1}{2}||w||+C\\sum_{i=1}^N \\xi_i-\\sum_{i=1}^N \\alpha_i (y_i(w \\cdot x_i+b-1+\\xi_i))-\\sum_{i=1}^N \\mu_i \\xi_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$\\alpha_i \\ge 0, \\mu_i \\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先求$L(w,b,\\xi,a,\\mu)$对$w,b,\\xi$的极小，得\n",
    "$$\n",
    "w=\\sum_{i=1}^N \\alpha_i y_i x_i \\\\\n",
    "\\sum_{i=1}^N \\alpha_i y_i \\\\\n",
    "C-\\alpha_i-\\mu_i=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对偶问题是**\n",
    "$$\n",
    "\\max_{\\alpha} -\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) +\\sum_{i=1}^N \\alpha_i \\\\\n",
    "s.t. \\quad \\sum_{i=1}^N \\alpha_i y_i =0 \\\\\n",
    "C-\\alpha_i-\\mu_i=0 \\\\\n",
    "\\alpha_i \\ge 0 \\\\\n",
    "\\mu_i \\ge 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过消去$\\mu$得到转换后的对偶问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合页损失函数\n",
    "线性支持向量机学习还有另外一种解释，就是最小化一下目标函数：\n",
    "$$\n",
    "\\sum_{i=1}^N[1-y_i(w \\cdot x_i+b)]_+ + \\lambda ||w||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一项为经验损失，称为合页损失函数（hinge loss function）\n",
    "$$\n",
    "L(y(w \\cdot x+b))=[1-y(w \\cdot x +b)]_+\n",
    "$$\n",
    "下标+表示取正值得函数\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "[z]_+=\n",
    "\\begin{cases}\n",
    "z, \\quad z\\gt 0 \\\\\n",
    "0, \\quad z\\le 0\n",
    "\\end{cases}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合页损失函数是0-1损失函数的上界"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 非线性支持向量机与核函数\n",
    "## 核函数定义\n",
    "设$\\mathcal X$是输入空间（欧氏空间$R^n$的子集或离散集合），又设$\\mathcal H$为特征空间（希尔伯特空间），如果存在一个从$\\mathcal X$ 到 $\\mathcal H$的映射\n",
    "$$\n",
    "\\Phi(x):\\mathcal X \\rightarrow \\mathcal H\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使得对所有$x,z \\in \\mathcal X$，函数$K(x,z)$满足条件：\n",
    "$$\n",
    "K(x,z)=\\phi(x) \\cdot \\phi(z)\n",
    "$$\n",
    "则称$K(x,z)$为核函数，$\\phi(x)$为映射函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将原来非线性可分的分类问题通过$\\Phi$映射到新的空间上，线性可分\n",
    "$$\n",
    "W(\\alpha)=\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\Phi(x_i) \\cdot \\Phi(x_j)-\\sum_{i=1}^N \\alpha_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用核函数替换得\n",
    "$$\n",
    "W(\\alpha)=\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i,x_j)-\\sum_{i=1}^N \\alpha_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习得到的分类函数为\n",
    "$$\n",
    "f(x)=sign(\\sum_{i=1}^N a_i^* y_iK(x_i,x)+b^*)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当映射函数为非线性函数时，学习到的含有核函数的支持向量机是非线性分类模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正定核\n",
    "假设$K(x,z)$是定义在$X^2$上的对称函数，并且对于任意的$x_1,x_2,\\ldots,x_m \\in X$，K(x,z)关于$x_1,x_2,\\ldots,x_m$的Gram矩阵是半正定的。可以根据函数$K(x,z)$，构成一个希尔伯特空间（Hilbert space）。\n",
    "### 正定核的充要条件（略）\n",
    "## 常用核函数\n",
    "### 多项式核函数（polynomial kernel function）\n",
    "$$\n",
    "K(x,z)=(x \\cdot z +1)^p\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 高斯核函数（Gaussian kernel function）\n",
    "$$\n",
    "K(x,z)=\\exp(-\\frac{||x-z||^2}{2\\sigma^2})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字符串核函数（string kernel function）\n",
    "略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非线性支持向量机\n",
    "从非线性分类训练集，通过核函数与软间隔最大化，或凸二次规划（7.95）~(7.97),学习得到的分类决策函数\n",
    "$$\n",
    "f(x)=sign(\\sum_{i=1}^N a_i^* y_i K(x,x_i)+b^*)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 序列最小最优化算法\n",
    "序列最小最优化算法（sequential minimal optimization, SMO）1998年由Platt提出。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
